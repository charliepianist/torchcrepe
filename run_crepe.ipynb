{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import pickle\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from scipy.io import wavfile\r\n",
        "\r\n",
        "import sys\r\n",
        "sys.path.append('/home/azureuser/cloudfiles/code/Users/cl43/torchcrepe')\r\n",
        "import torchcrepe\r\n",
        "from torchcrepe.predict_custom import predict as predict_custom\r\n",
        "from torchcrepe.predict_custom import load_audio"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1641622896565
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load audio"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = 'data/test/CSD/wav/converted'\r\n",
        "pitch_path = 'out/pitch'\r\n",
        "num_files = 60 # How many data points to evaluate on, including skipped files? (Set to -1 to evaluate on all available files)\r\n",
        "skip_files = ['en002a', 'en002b', 'en004a', 'en005a', 'en005b', 'en007a', 'en007b', 'en008a', 'en008b',  # 6\r\n",
        "                'en009a', 'en009b', 'en010a', 'en010b', 'en011a', 'en012a', 'en012b', 'en013b', 'en014a', 'en014b', # 2\r\n",
        "                'en015a', 'en016a', 'en018a', 'en018b', 'en019b', 'en020a', 'en020b', 'en021a', 'en022a', 'en022b', 'en023a', 'en023b', 'en024a', # 7\r\n",
        "                'en025a', 'en025b', 'en026a', 'en026b', 'en027a', 'en027b' # 6 (en028 and en029 and en030)\r\n",
        "                ] \r\n",
        "\r\n",
        "model = 'kl_full'\r\n",
        "capacity = 'full'\r\n",
        "\r\n",
        "# e.g. out/pitch/base_tiny\r\n",
        "full_pitch_path = os.path.join(pitch_path, model)\r\n",
        "\r\n",
        "# Read in wavfiles\r\n",
        "raw_filenames = []\r\n",
        "wavfiles = [] # (sample rate, audio data)\r\n",
        "i = 0\r\n",
        "for filename in os.listdir(data_path):\r\n",
        "    if filename.endswith(\".wav\"):\r\n",
        "        i += 1\r\n",
        "        # limit # files\r\n",
        "        if i > num_files and num_files > 0:\r\n",
        "            break\r\n",
        "\r\n",
        "        # Skip undesired files\r\n",
        "        if filename[:-4] in skip_files:\r\n",
        "            continue\r\n",
        "\r\n",
        "        # Skip files that were already done\r\n",
        "        raw_filename = filename[:-4]\r\n",
        "        saved_filename = os.path.join(full_pitch_path, raw_filename + '.pkl')\r\n",
        "        if os.path.exists(saved_filename):\r\n",
        "            print(saved_filename, 'already exists')\r\n",
        "            continue\r\n",
        "        \r\n",
        "        # print(os.path.join(data_path, filename))\r\n",
        "        raw_filenames.append(raw_filename)\r\n",
        "        audio, sr = load_audio(os.path.join(data_path, filename))\r\n",
        "        wavfiles.append((audio, sr))\r\n",
        "        # print(wavfiles[-1])\r\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1641622904703
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run predictions"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run/load crepe predictions\r\n",
        "def save_obj(filename, objs):\r\n",
        "    with open(filename, 'wb') as outp:\r\n",
        "        for obj in objs:\r\n",
        "            pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "def read_obj(filename, num_to_read):\r\n",
        "    objs = []\r\n",
        "    with open(filename, 'rb') as inp:\r\n",
        "        for i in range(num_to_read):\r\n",
        "            objs.append(pickle.load(inp))\r\n",
        "        return objs\r\n",
        "\r\n",
        "times = []\r\n",
        "frequencies = []\r\n",
        "confidences = []\r\n",
        "activations = []\r\n",
        "# Run/load crepe predictions\r\n",
        "for i, file_info in enumerate(wavfiles):\r\n",
        "    filename = os.path.join(full_pitch_path, raw_filenames[i] + '.pkl')\r\n",
        "    print('audio shape:', file_info[0].shape)\r\n",
        "    if os.path.exists(filename):\r\n",
        "        # Read cached prediction outputs\r\n",
        "        arr = read_obj(filename, 4)\r\n",
        "        frequency = arr[1]\r\n",
        "        confidence = arr[2]\r\n",
        "        activation = arr[3]\r\n",
        "        print(filename, 'already exists')\r\n",
        "    else:\r\n",
        "        # Run prediction and save output\r\n",
        "        sr = file_info[1]\r\n",
        "        audio = file_info[0]\r\n",
        "        # print(sr, audio)\r\n",
        "        # print(audio.detach().cpu().numpy().squeeze(0))\r\n",
        "        # print(audio)\r\n",
        "        # hop length 20ms\r\n",
        "        time, frequency, confidence, activation = predict_custom(audio, sr, hop_length=sr/50, model=model, decoder=torchcrepe.decode.weighted_argmax, capacity=capacity, special='KL')\r\n",
        "        save_obj(filename, [time, frequency, confidence, activation]) # Uncomment to save predictions \r\n",
        "        print('saved to', filename)\r\n",
        "    \r\n",
        "    # # freq = 27.5 * 2 ** ((msg.note - 21) / 12) \r\n",
        "    # # Convert frequency back to note -> note = 12 * log_2(freq / 27.5) + 21\r\n",
        "    # if not use_frequency:\r\n",
        "    #     for idx in range(len(frequency)):\r\n",
        "    #         frequency[idx] = 12 * np.log2(frequency[idx] / 27.5) + 21\r\n",
        "    #         if not frequency[idx] >= 0:\r\n",
        "    #             print(frequency[idx])\r\n",
        "\r\n",
        "    # print(frequency, confidence, activation)\r\n",
        "    # print(frequency.shape)\r\n",
        "    # import torch\r\n",
        "    # print(torch.mean(frequency))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "audio shape: torch.Size([1, 2857680])\nsaved to out/pitch/kl_full/en001a.pkl\naudio shape: torch.Size([1, 2857680])\nsaved to out/pitch/kl_full/en001b.pkl\naudio shape: torch.Size([1, 2761044])\nsaved to out/pitch/kl_full/en003a.pkl\naudio shape: torch.Size([1, 2761044])\nsaved to out/pitch/kl_full/en003b.pkl\naudio shape: torch.Size([1, 2822400])\nsaved to out/pitch/kl_full/en004b.pkl\naudio shape: torch.Size([1, 3880801])\nsaved to out/pitch/kl_full/en006a.pkl\naudio shape: torch.Size([1, 3880801])\nsaved to out/pitch/kl_full/en006b.pkl\naudio shape: torch.Size([1, 4795875])\nsaved to out/pitch/kl_full/en011b.pkl\naudio shape: torch.Size([1, 1984500])\nsaved to out/pitch/kl_full/en013a.pkl\naudio shape: torch.Size([1, 3024000])\nsaved to out/pitch/kl_full/en015b.pkl\naudio shape: torch.Size([1, 3112941])\nsaved to out/pitch/kl_full/en016b.pkl\naudio shape: torch.Size([1, 840001])\nsaved to out/pitch/kl_full/en017a.pkl\naudio shape: torch.Size([1, 840001])\nsaved to out/pitch/kl_full/en017b.pkl\naudio shape: torch.Size([1, 3573819])\nsaved to out/pitch/kl_full/en019a.pkl\naudio shape: torch.Size([1, 3598561])\nsaved to out/pitch/kl_full/en021b.pkl\naudio shape: torch.Size([1, 3373650])\nsaved to out/pitch/kl_full/en024b.pkl\naudio shape: torch.Size([1, 4731671])\nsaved to out/pitch/kl_full/en028a.pkl\naudio shape: torch.Size([1, 4731671])\nsaved to out/pitch/kl_full/en028b.pkl\naudio shape: torch.Size([1, 2681280])\nsaved to out/pitch/kl_full/en029a.pkl\naudio shape: torch.Size([1, 2681280])\nsaved to out/pitch/kl_full/en029b.pkl\naudio shape: torch.Size([1, 3175200])\nsaved to out/pitch/kl_full/en030a.pkl\naudio shape: torch.Size([1, 3175200])\nsaved to out/pitch/kl_full/en030b.pkl\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1641626205108
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}